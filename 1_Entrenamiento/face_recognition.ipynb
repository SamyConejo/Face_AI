{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42cf7855",
   "metadata": {},
   "source": [
    "\n",
    "**autor:** @SamyConejo\n",
    "\n",
    "\n",
    "*   **Tema:** Prototipo de sistema móvil para reconocimiento facial aplicando Redes Neuronales Convolucionales y Transferencia de Aprendizaje.\n",
    "\n",
    "\n",
    "***Descripción***\n",
    "1.  Configuración de la RNC MobileNetV2 para aplicar Transfer Learning y exportar el modelo en formato .tflite para implementación en dispositivo móvil.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b82269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import MobileNetV2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix, classification_report\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from statistics import mean\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f768ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to dataset\n",
    "base_dir = '/Users/samcn96/PycharmProjects/training/rostros/dataset'\n",
    "train_data_dir='/Users/samcn96/PycharmProjects/training/rostros/dataset/training'\n",
    "validation_data_dir='/Users/samcn96/PycharmProjects/training/rostros/dataset/validation'\n",
    "test_path='/Users/samcn96/PycharmProjects/training/rostros/dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb872fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_SIZE = 224 # image size\n",
    "BATCH_SIZE = 64  # number of images we are inputting into the neural network at once.\n",
    "EPOCHS = 100 # number of epochs to train\n",
    "NUM_CLASSES = 10 # we have 10 people to predict\n",
    "\n",
    "# scalling images to [0 - 1]\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory( # training generator\n",
    "    train_data_dir,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True\n",
    ")\n",
    "val_generator = datagen.flow_from_directory(  #validation generator\n",
    "    validation_data_dir,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save labes.txt to use later in App Development\n",
    "\n",
    "print(train_generator.class_indices)\n",
    "labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n",
    "with open('labels.txt', 'w') as f:\n",
    "    f.write(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e24814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV2, not including top layers to enable Transfer Learning\n",
    "\n",
    "MobileNet = MobileNetV2(weights='imagenet',\n",
    "                      include_top=False,\n",
    "                      input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "# We freeze layers, except 8 last layers.\n",
    "count = 0\n",
    "for layer in MobileNet.layers:\n",
    "    if count < 146:\n",
    "        layer.trainable = False\n",
    "    count += 1\n",
    "\n",
    "# print layers to check freeze    \n",
    "for (i, layer) in enumerate(MobileNet.layers):\n",
    "    print(str(i) + \" \" + layer.__class__.__name__, layer.trainable)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa796898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our top custom layers.\n",
    "def lw(bottom_model, num_classes):\n",
    "    top_model = bottom_model.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "    top_model = Dense(256,activation='relu')(top_model)\n",
    "    top_model = Dense(NUM_CLASSES,activation='softmax')(top_model)\n",
    "    return top_model\n",
    "\n",
    "FC_Head = lw(MobileNet, NUM_CLASSES)\n",
    "model = Model(inputs = MobileNet.input, outputs = FC_Head)\n",
    "\n",
    "# print our custom model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2820c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use callbacks to control training process\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"face_recognized.h5\",\n",
    "                             monitor=\"val_accuracy\",\n",
    "                             mode=\"max\",\n",
    "                             save_best_only = True,\n",
    "                             period=1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_accuracy',\n",
    "                              factor = 0.5,\n",
    "                              patience = 2,\n",
    "                              verbose = 1,\n",
    "                              mode = 'max',\n",
    "                              min_lr = 0.000001)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_accuracy',\n",
    "                          patience = 5,\n",
    "                          verbose = 1,\n",
    "                          mode = 'max',\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "\n",
    "callbacks = [earlystop, reduce_lr, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d112b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and fit model with our configuration, we use lr = 10-4, Adam optimizer.\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs = EPOCHS,\n",
    "    callbacks = callbacks,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d66078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Accuracy\n",
    "\n",
    "print('Mean Training Accuracy ', mean(history.history['accuracy']))\n",
    "print('Mean Validation Acurracy ', mean(history.history['val_accuracy']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a6a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save history\n",
    "data = str(history.history)\n",
    "with open('history.txt', 'w') as f:\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db08b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we plot performance of training and validation.\n",
    "\n",
    "plt.plot(history.history['loss'], label = 'Training')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation')\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss.png')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label = 'Training')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Validation')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('accuracy.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df25d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS =[\"Elvis\", \"Fausto\", \"Ale\", 'Jenifer', 'Malki', 'Maru', 'Nayta', 'Roberto','Samy', 'Sulay']\n",
    "\n",
    "def my_metrics(y_true, y_pred):\n",
    "  \n",
    "    \n",
    "    accuracy=accuracy_score(y_true, y_pred)\n",
    "    precision=precision_score(y_true, y_pred,average='weighted')\n",
    "    f1Score=f1_score(y_true, y_pred, average='weighted') \n",
    "    print(\"Accuracy  : {}\".format(accuracy))\n",
    "    print(\"Precision : {}\".format(precision))\n",
    "    print(\"F1 Score  : {}\".format(f1Score))\n",
    "    cm=confusion_matrix(y_true, y_pred)\n",
    "   \n",
    "    report = classification_report(y_true, y_pred, labels=[0,1,2,3,4,5,6,7,8,9], \n",
    "                                   target_names=LABELS)\n",
    "    print(report)\n",
    "    return accuracy, precision, f1Score, cm\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=False):\n",
    " \n",
    "    \n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.savefig('matrix.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e7a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model over test folder\n",
    "print(\"==============TEST RESULTS============\")\n",
    "test_generator = datagen.flow_from_directory(\n",
    "        test_path,\n",
    "        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode = None,\n",
    "        shuffle=False) \n",
    "predictions = model.predict(test_generator, verbose=1)\n",
    "yPredictions = np.argmax(predictions, axis=1)\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "testAcc,testPrec, testFScore, cm = my_metrics(true_classes, yPredictions)\n",
    "\n",
    "plot_confusion_matrix(cm, LABELS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can export the model\n",
    "\n",
    "saved_model_dir = 'mobile'\n",
    "tf.saved_model.save(model, saved_model_dir) \n",
    "\n",
    "# we converte .h5 model to .tflite to mobile on device implementation.\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tflite_model = converter.convert() \n",
    "\n",
    "with open('mobile/model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have our model.h5 and model.tflite, and lets test on testing files.\n",
    "\n",
    "# dic to map output of model inference and labels\n",
    "my_face_dict =  { \"[0]\": \"Elvis\",\n",
    "                  \"[1]\": \"Fausto\",\n",
    "                  \"[2]\": \"Ale\",\n",
    "                  \"[3]\": \"Jenifer\",\n",
    "                  \"[4]\": \"Malki\",\n",
    "                  \"[5]\": \"Maru\",\n",
    "                  \"[6]\": \"Nayta\",\n",
    "                  \"[7]\": \"Roberto\",\n",
    "                  \"[8]\": \"Samy\",\n",
    "                  \"[9]\": \"Sulay\",\n",
    "                      }\n",
    "# dic to print class label\n",
    "my_face_dict_n = {\"P0\": \"Elvis\",\n",
    "                  \"P1\": \"Fausto\",\n",
    "                  \"P2\": \"Ale\",\n",
    "                  \"P3\": \"Jenifer\",\n",
    "                  \"P4\": \"Malki\",\n",
    "                  \"P5\": \"Maru\",\n",
    "                  \"P6\": \"Nayta\",\n",
    "                  \"P7\": \"Roberto\",\n",
    "                  \"P8\": \"Samy\",\n",
    "                  \"P9\": \"Sulay\",\n",
    "                    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23828d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to show image with label.\n",
    "def draw_test(name, pred, im):\n",
    "    myface = my_face_dict[str(pred)]\n",
    "    BLACK = [0, 0, 0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 80, 0, 0, 100, cv2.BORDER_CONSTANT, value=BLACK)\n",
    "    cv2.putText(expanded_image, myface, (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.imshow('test', expanded_image)\n",
    "\n",
    "# select random images from testing folder, returns path to image.\n",
    "def getRandomImage(path):\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0, len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    print(\"Class - \" + my_face_dict_n[str(path_class)])\n",
    "    file_path = path + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0, len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    return cv2.imread(file_path + \"/\" + image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets test with 50 random images to check model.tflite\n",
    "for i in range(0, 50):\n",
    "    input_im = getRandomImage(\"/Users/samcn96/PycharmProjects/training/rostros/test/\")\n",
    "    input_original = input_im.copy()\n",
    "    input_original = cv2.resize(input_original, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # check image size and resize if needed\n",
    "    input_im = cv2.resize(input_im, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "    input_im = input_im / 255\n",
    "    input_im = input_im.reshape(1, 224, 224, 3)\n",
    "\n",
    "    # Load TFLite model and allocate tensors.\n",
    "    interpreter = tf.lite.Interpreter(model_path=\"mobile/model.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Test model on random input data.\n",
    "    input_shape = input_details[0]['shape']\n",
    "\n",
    "    input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "    \n",
    "    # predict on input_image\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_im.astype(np.float32))\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    res = np.argmax(output_data, axis=1)\n",
    "    \n",
    "    # print class prediction\n",
    "    print(res)\n",
    "    \n",
    "    # Show image with predicted class\n",
    "    draw_test(\"Prediction\", res, input_original)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
